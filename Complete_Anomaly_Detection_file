#DATA
import os
from os.path import isdir
import tarfile
import wget
import ssl
from pathlib import Path
from PIL import Image

from torch import tensor
from torchvision.datasets import ImageFolder
from torchvision import transforms
from torch.utils.data import DataLoader


# Parameters
DATASETS_PATH = Path("./datasets")
DEFAULT_SIZE = 224
DEFAULT_RESIZE = 256

IMAGENET_MEAN = tensor([.485, .456, .406])
IMAGENET_STD = tensor([.229, .224, .225])
CLIP_MEAN = tensor([.481, .457, .408])
CLIP_STD = tensor([.268, .261, .275])

class_links = {
    "bottle": "https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420937370-1629951468/bottle.tar.xz",
    "cable": "https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420937413-1629951498/cable.tar.xz",
    "capsule": "https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420937454-1629951595/capsule.tar.xz",
    "carpet": "https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420937484-1629951672/carpet.tar.xz",
    "grid": "https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420937487-1629951814/grid.tar.xz",
    "hazelnut": "https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420937545-1629951845/hazelnut.tar.xz",
    "leather": "https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420937607-1629951964/leather.tar.xz",
    "metal_nut": "https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420937637-1629952063/metal_nut.tar.xz",
    "pill": "https://www.mydrive.ch/shares/43421/11a215a5749fcfb75e331ddd5f8e43ee/download/420938129-1629953099/pill.tar.xz",
    "screw": "https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420938130-1629953152/screw.tar.xz",
    "tile": "https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420938133-1629953189/tile.tar.xz",
    "toothbrush": "https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420938134-1629953256/toothbrush.tar.xz",
    "transistor": "https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420938166-1629953277/transistor.tar.xz",
    "wood": "https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420938383-1629953354/wood.tar.xz",
    "zipper": "https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420938385-1629953449/zipper.tar.xz"
}



def mvtec_classes():
    return [
        "metal_nut",
        "screw"
    ]


class MVTecDataset:
    def __init__(
            self,
            cls: str,
            size: int = DEFAULT_SIZE,
            vanilla: bool = True,
    ):
        assert cls in mvtec_classes()

        # Parameters
        self.cls = cls
        self.size = size

        # Download data
        self.check_and_download_cls()

        # Build datasets
        if vanilla:
            resize = DEFAULT_RESIZE
        else:   # CLIP
            resize = size

        self.train_ds = MVTecTrainDataset(cls, size, resize, vanilla)
        self.test_ds = MVTecTestDataset(cls, size, resize, vanilla)


    def check_and_download_cls(self):
        """
            If the expected dataset path is not found,
            download the dataset inside /dataset.
            by Sharvesh Subhash
        """

        if not isdir(DATASETS_PATH / self.cls):
            print(f"Class '{self.cls}' has not been found in '{DATASETS_PATH}/'. Downloading... \n")

            ssl._create_default_https_context = ssl._create_unverified_context
            wget.download(class_links[self.cls])
            with tarfile.open(f"{self.cls}.tar.xz") as tar: # Unzip
                tar.extractall(DATASETS_PATH)
            os.remove(f"{self.cls}.tar.xz") # Clean up

            print(f"Correctly Downloaded \n")

        else:
            print(f"Class '{self.cls}' has been found in '{DATASETS_PATH}/'\n")


    def get_datasets(self):
        """
            Returns as tuple:
            - train dataset (MVTecTrainDataset class)
            - test dataset (MVTecTestDataset class)
            by Sharvesh Subhash
        """
        return self.train_ds, self.test_ds


    def get_dataloaders(self):
        """
            Returns as tuple:
            - train dataloader (torch.utils.data.DataLoader class)
            - test dataloader (torch.utils.data.DataLoader class)
            by Sharvesh Subhash
        """
        return DataLoader(self.train_ds), DataLoader(self.test_ds)


def _convert_image_to_rgb(image):
    return image.convert("RGB")


class MVTecTrainDataset(ImageFolder):
    def __init__(
            self,
            cls: str,
            size: int,
            resize: int = DEFAULT_RESIZE,
            vanilla: bool = True,
    ):
        # Vanilla/CLIP image pre-processing
        if vanilla:
            transform = transforms.Compose([
                transforms.Resize(resize),
                transforms.CenterCrop(size),
                transforms.ToTensor(),
                transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),
            ])
        else:
            transform = transforms.Compose([
                transforms.Resize(resize, interpolation=transforms.InterpolationMode.BICUBIC),
                transforms.CenterCrop(size),
                _convert_image_to_rgb,
                transforms.ToTensor(),
                transforms.Normalize(CLIP_MEAN, CLIP_STD),
            ])

        # Parameters
        super().__init__(
                root = DATASETS_PATH / cls / "train",
                transform = transform )
        self.cls = cls
        self.size = size


class MVTecTestDataset(ImageFolder):
    def __init__(
            self,
            cls: str,
            size: int,
            resize: int = DEFAULT_RESIZE,
            vanilla: bool = True,
    ):

        # Vanilla/CLIP image and mask pre-processing
        if vanilla:
            transform = transforms.Compose([         # Image transform
                transforms.Resize(resize, interpolation=transforms.InterpolationMode.BICUBIC),
                transforms.CenterCrop(size),
                transforms.ToTensor(),
                transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),
            ])
            target_transform = transforms.Compose([  # Mask transform
                transforms.Resize(resize, interpolation=transforms.InterpolationMode.NEAREST),
                transforms.CenterCrop(size),
                transforms.ToTensor(),
            ])
        else:
            transform  = transforms.Compose([        # Image transform
                transforms.Resize(resize, interpolation=transforms.InterpolationMode.BICUBIC),
                transforms.CenterCrop(size),
                _convert_image_to_rgb,
                transforms.ToTensor(),
                transforms.Normalize(CLIP_MEAN, CLIP_STD),
            ])

            target_transform = transforms.Compose([  # Mask transform
                transforms.Resize(resize, interpolation=transforms.InterpolationMode.NEAREST),
                transforms.CenterCrop(size),
                _convert_image_to_rgb,
                transforms.ToTensor(),
            ])

        # Parameters
        super().__init__(
            root=DATASETS_PATH / cls / "test",
            transform = transform,
            target_transform = target_transform
        )

        self.cls = cls
        self.size = size


    def __getitem__(self, index):
        path, _ = self.samples[index]
        sample = self.loader(path)

        if "good" in path:                                      # Nominal image
            mask = Image.new('L', (self.size, self.size))       # L -> 8-bit pixels black and white
            sample_class = 0
        else:                                                   # Anomaly image
            mask_path = path.replace("test", "ground_truth")    # Change folder and goes into mask folder
            mask_path = mask_path.replace(".png", "_mask.png")  # Change extension required
            mask = self.loader(mask_path)                       # Load the mask
            sample_class = 1

        # Trasnformations
        if self.transform is not None:
            sample = self.transform(sample)

        if self.target_transform is not None:
            mask = self.target_transform(mask)

        return sample, mask[:1], sample_class
#UTIL
import torch
from torch import tensor
from torchvision import transforms

import numpy as np
import PIL
from PIL import ImageFilter
from sklearn import random_projection
from tqdm import tqdm



backbones = {
    'WideResNet50':'wide_resnet50_2',
    'ResNet101':'RN101',
    'ResNet50':'RN50',
    'ResNet50-4':'RN50x4',
    'ResNet50-16':'RN50x16',
}

dataset_scale_factor = {
    'WideResNet50': 1,
    'ResNet101': 1,
    'ResNet50': 1,
    'ResNet50-4': 2,
    'ResNet50-16': 4,
}

def get_coreset(
        memory_bank: tensor,
        l: int = 1000,  # Coreset target
        eps: float = 0.09,
) -> tensor:
    """
        Returns l coreset indexes for given memory_bank.

        Args:
        - memory_bank:     Patchcore memory bank tensor
        - l:               Number of patches to select
        - eps:             Sparse Random Projector parameter

        Returns:
        - coreset indexes
    """

    coreset_idx = []  # Returned coreset indexes
    idx = 0

    # Fitting random projections
    try:
        transformer = random_projection.SparseRandomProjection(eps=eps)
        memory_bank = torch.tensor(transformer.fit_transform(memory_bank))
    except ValueError:
        print("Error: could not project vectors. Please increase `eps`.")

    # Coreset subsampling
    print(f'Start Coreset Subsampling...')

    last_item = memory_bank[idx: idx + 1]   # First patch selected = patch on top of memory bank
    coreset_idx.append(torch.tensor(idx))
    min_distances = torch.linalg.norm(memory_bank - last_item, dim=1, keepdims=True)    # Norm l2 of distances (tensor)

    # Use GPU if possible
    if torch.cuda.is_available():
        last_item = last_item.to("cuda")
        memory_bank = memory_bank.to("cuda")
        min_distances = min_distances.to("cuda")

    for _ in tqdm(range(l - 1)):
        distances = torch.linalg.norm(memory_bank - last_item, dim=1, keepdims=True)    # L2 norm of distances (tensor)
        min_distances = torch.minimum(distances, min_distances)                         # Verical tensor of minimum norms
        idx = torch.argmax(min_distances)                                               # Index of maximum related to the minimum of norms

        last_item = memory_bank[idx: idx + 1]   # last_item = maximum patch just found
        min_distances[idx] = 0                  # Zeroing last_item distances
        coreset_idx.append(idx.to("cpu"))       # Save idx inside the coreset

    return torch.stack(coreset_idx)


def gaussian_blur(img: tensor) -> tensor:
    """
        Apply a gaussian smoothing with sigma = 4 over the input image.
    """
    # Setup
    blur_kernel = ImageFilter.GaussianBlur(radius=4)
    tensor_to_pil = transforms.ToPILImage()
    pil_to_tensor = transforms.ToTensor()

    # Smoothing
    max_value = img.max()   # Maximum value of all elements in the image tensor
    blurred_pil = tensor_to_pil(img[0] / max_value).filter(blur_kernel)
    blurred_map = pil_to_tensor(blurred_pil) * max_value

    return blurred_map


def tensor_to_image(tensor):
    tensor = tensor * 255
    tensor = np.array(tensor, dtype=np.uint8)

    if np.ndim(tensor) > 3:
        assert tensor.shape[0] == 1
        tensor = tensor[0]

    return PIL.Image.fromarray(tensor)


def display_backbones():
    vanilla = True
    print("Vanilla PatchCore backbone:")
    print(f"- WideResNet50")
    print("CLIP Image Encoder architectures for PatchCore backbone:")
    for k, _ in backbones.items():
        if vanilla:
            vanilla = False
            continue
        print(f"- {k}")
    print()


def display_MVTec_classes():
    print(mvtec_classes())
#PatchCore
import torch
from torch import tensor
from torch.utils.data import DataLoader
from torch.nn import functional as F
import torchvision
import torchvision.transforms as T

from tqdm import tqdm
import clip
from PIL import Image
import numpy as np
from sklearn.metrics import roc_auc_score



class PatchCore(torch.nn.Module):
    def __init__(
            self,
            f_coreset:float = 0.01,    # Fraction rate of training samples
            eps_coreset: float = 0.90, # SparseProjector parameter
            k_nearest: int = 3,        # k parameter for K-NN search
            vanilla: bool = True,      # if False, use CLIP
            backbone: str = 'wide_resnet50_2',
            image_size: int = 224
    ):
        assert f_coreset > 0
        assert eps_coreset > 0
        assert k_nearest > 0
        assert image_size > 0

        super(PatchCore, self).__init__()

        if vanilla:
            print(f"Vanilla Mode")
        else:
            print(f"CLIP Mode")
        print(f"Net Used: {backbone}")

        # Hook to extract feature maps
        def hook(module, input, output) -> None:
            """This hook saves the extracted feature map on self.featured."""
            self.features.append(output)

        # Register hooks
        if vanilla == True:
            self.model = torch.hub.load('pytorch/vision:v0.13.0', 'wide_resnet50_2', pretrained=True)
            self.model.layer2[-1].register_forward_hook(hook)
            self.model.layer3[-1].register_forward_hook(hook)
        else:
            self.model, _ = clip.load(backbone, device="cpu")
            self.model.visual.layer2[-1].register_forward_hook(hook)
            self.model.visual.layer3[-1].register_forward_hook(hook)

        # Disable gradient computation
        self.model.eval()
        for param in self.model.parameters():
            param.requires_grad = False

        # Parameters
        self.memory_bank = []
        self.f_coreset = f_coreset      # Fraction rate of training samples
        self.eps_coreset = eps_coreset  # SparseProjector parameter
        self.k_nearest = k_nearest      # k parameter for K-NN search
        self.vanilla = vanilla          # True: vanilla PatchCore, False: CLIP PatchCore
        self.backbone = backbone
        self.image_size = image_size


    def forward(self, sample: tensor):
        """
            Initialize self.features and let the input sample passing
            throught the backbone net self.model.
            The registered hooks will extract the layer 2 and 3 feature maps.
            Return:
                self.feature filled with extracted feature maps
                by Sharvesh Subhash
        """

        self.features = []
        if self.vanilla:
            _ = self.model(sample)
        else:
            _ = self.model.visual(sample)  # Clip

        return self.features


    def fit(self, train_dataloader: DataLoader, scale: int=1) -> None:

        """
            Training phase
            Creates memory bank from train dataset and apply greedy coreset subsampling.
            By Sharvesh Subhash
        """
        tot = len(train_dataloader) // scale
        counter = 0

        for sample, _ in tqdm(train_dataloader, total=tot):
            feature_maps = self(sample)                   # Extract feature maps

            # Create aggregation function of feature vectors in the neighbourhood
            self.avg = torch.nn.AvgPool2d(3, stride=1)
            fmap_size = feature_maps[0].shape[-2]         # Feature map sizes h, w
            self.resize = torch.nn.AdaptiveAvgPool2d(fmap_size)

            # Create patch
            resized_maps = [self.resize(self.avg(fmap)) for fmap in feature_maps]
            patch = torch.cat(resized_maps, 1)            # Merge the resized feature maps
            patch = patch.reshape(patch.shape[1], -1).T   # Create a column tensor

            self.memory_bank.append(patch)                # Fill memory bank
            counter += 1
            if counter > tot:
                break

        self.memory_bank = torch.cat(self.memory_bank, 0) # VStack the patches

        # Coreset subsampling
        if self.f_coreset < 1:
            coreset_idx = get_coreset(
                self.memory_bank,
                l = int(self.f_coreset * self.memory_bank.shape[0]),
                eps = self.eps_coreset
            )
            self.memory_bank = self.memory_bank[coreset_idx]


    def evaluate(self, test_dataloader: DataLoader):
        """
            Compute anomaly detection score and relative segmentation map for
            each test sample. Returns the ROC AUC computed from predictions scores.

            Returns:
            - image-level ROC-AUC score
            - pixel-level ROC-AUC score
            by Sharvesh Subhash
        """

        image_preds = []
        image_labels = []
        pixel_preds = []
        pixel_labels = []

        for sample, mask, label in tqdm(test_dataloader):

            image_labels.append(label)
            pixel_labels.extend(mask.flatten().numpy())

            score, segm_map = self.predict(sample)  # Anomaly Detection

            image_preds.append(score.numpy())
            pixel_preds.extend(segm_map.flatten().numpy())

        image_labels = np.stack(image_labels)
        image_preds = np.stack(image_preds)

        # Compute ROC AUC for prediction scores
        image_level_rocauc = roc_auc_score(image_labels, image_preds)
        pixel_level_rocauc = roc_auc_score(pixel_labels, pixel_preds)

        return image_level_rocauc, pixel_level_rocauc


    def predict(self, sample: tensor):
        """
            Anomaly detection over a test sample
            1. Create a locally aware patch feature of the test sample.
            2. Compute the image-level anomaly detection score by comparing
            the test patch with the nearest neighbours patches inside the memory bank.
            3. Compute a segmentation map realigning computed path anomaly scores based on
            their respective spacial location. Then upscale the segmentation map by
            bi-linear interpolation and smooth the result with a gaussian blur.

            Args:
            - sample:  test sample

            Returns:
            - Segmentation score
            - Segmentation map
            by Sharvesh Subhash
        """

        # Patch extraction
        feature_maps = self(sample)
        resized_maps = [self.resize(self.avg(fmap)) for fmap in feature_maps]
        patch = torch.cat(resized_maps, 1)
        patch = patch.reshape(patch.shape[1], -1).T

        # Compute maximum distance score s* (equation 6 from the paper)
        distances = torch.cdist(patch, self.memory_bank, p=2.0)         # L2 norm dist btw test patch with each patch of memory bank
        dist_score, dist_score_idxs = torch.min(distances, dim=1)       # Val and index of the distance scores (minimum values of each row in distances)
        s_idx = torch.argmax(dist_score)                                # Index of the anomaly candidate patch
        s_star = torch.max(dist_score)                                  # Maximum distance score s*
        m_test_star = torch.unsqueeze(patch[s_idx], dim=0)              # Anomaly candidate patch
        m_star = self.memory_bank[dist_score_idxs[s_idx]].unsqueeze(0)  # Memory bank patch closest neighbour to m_test_star

        # KNN
        knn_dists = torch.cdist(m_star, self.memory_bank, p=2.0)        # L2 norm dist btw m_star with each patch of memory bank
        _, nn_idxs = knn_dists.topk(k=self.k_nearest, largest=False)    # Values and indexes of the k smallest elements of knn_dists

        # Compute image-level anomaly score s
        m_star_neighbourhood = self.memory_bank[nn_idxs[0, 1:]]
        w_denominator = torch.linalg.norm(m_test_star - m_star_neighbourhood, dim=1)    # Sum over the exp of l2 norm distances btw m_test_star and the m* neighbourhood
        norm = torch.sqrt(torch.tensor(patch.shape[1]))                                 # Softmax normalization trick to prevent exp(norm) from becoming infinite
        w = 1 - (torch.exp(s_star / norm) / torch.sum(torch.exp(w_denominator / norm))) # Equation 7 from the paper
        s = w * s_star

        # Segmentation map
        fmap_size = feature_maps[0].shape[-2:]          # Feature map sizes: h, w
        segm_map = dist_score.view(1, 1, *fmap_size)    # Reshape distance scores tensor
        segm_map = torch.nn.functional.interpolate(     # Upscale by bi-linaer interpolation to match the original input resolution
                        segm_map,
                        size=(self.image_size, self.image_size),
                        mode='bilinear'
                    )
        segm_map = gaussian_blur(segm_map)              # Gaussian blur of kernel width = 4

        return s, segm_map
#MAIN_RUNNING
ALL_CLASSES = mvtec_classes()
def run_model(
        classes: list = ALL_CLASSES,
        backbone: str = 'WideResNet50'
) -> None:

    f_coreset = 0.1

    # Vanilla or Clip version
    vanilla = backbone == "WideResNet50"

    results = {}    # key = class, Value = [image-level ROC AUC, pixel-level ROC AUC]
    if vanilla:
        size = DEFAULT_SIZE
    elif backbone == 'ResNet50':    # RN50
        size = 224
    elif backbone == 'ResNet50-4':  # RN50x4
        size = 288
    elif backbone == 'ResNet50-16': # RN50x16
        size = 384
    elif backbone == 'ResNet101':   # RN50x101
        size = 224
    else:
        raise Exception('You can use the following nets: ResNet50, ResNet50-4, ResNet50-16, ResNet50-64, ResNet101')

    print(f'Running PatchCore...')
    for cls in classes:
        train_dl, test_dl = MVTecDataset(cls, size=size, vanilla=vanilla).get_dataloaders()
        patch_core = PatchCore(f_coreset, vanilla=vanilla, backbone=backbones[backbone], image_size=size)

        print(f'\nClass {cls}:')
        print(f'Training...')
        patch_core.fit(train_dl, scale=dataset_scale_factor[backbone])

        print(f'Testing...')
        image_rocauc, pixel_rocauc = patch_core.evaluate(test_dl)

        print(f'Results:')
        results[cls] = [float(image_rocauc), float(pixel_rocauc)]
        print(f'- Image-level ROC AUC = {image_rocauc:.3f}')
        print(f'- Iixel-level ROC AUC = {pixel_rocauc:.3f}\n')

    # Save global results and statistics
    image_results = [v[0] for k, v in results.items()]
    average_image_rocauc = sum(image_results) / len(image_results)
    pixel_resuts = [v[1] for k, v in results.items()]
    average_pixel_rocauc = sum(pixel_resuts) / len(pixel_resuts)

    print(f'- Average image-level ROC AUC = {average_image_rocauc:.3f}\n')
    print(f'- Average pixel-level ROC AUC = {average_pixel_rocauc:.3f}\n')



if __name__ == "__main__":
    run_model(backbone='WideResNet50')

#Module for running
from pathlib import Path
import numpy as np
import os, shutil
import matplotlib.pyplot as plt

from PIL import Image

from tqdm.auto import tqdm

import torch
import torchvision
from torch.utils.data import DataLoader
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
import torch.optim as optim

from torchvision.models import resnet50, ResNet50_Weights
transform = transforms.Compose([
    transforms.Resize((224,224)),
    transforms.ToTensor()
])
class resnet_feature_extractor(torch.nn.Module):
    def __init__(self):
        """This class extracts the feature maps from a pretrained Resnet model."""
        super(resnet_feature_extractor, self).__init__()
        self.model = resnet50(weights=ResNet50_Weights.DEFAULT)

        self.model.eval()
        for param in self.model.parameters():
            param.requires_grad = False



        # Hook to extract feature maps
        def hook(module, input, output) -> None:
            """This hook saves the extracted feature map on self.featured."""
            self.features.append(output)

        self.model.layer2[-1].register_forward_hook(hook)
        self.model.layer3[-1].register_forward_hook(hook)

    def forward(self, input):

        self.features = []
        with torch.no_grad():
            _ = self.model(input)

        self.avg = torch.nn.AvgPool2d(3, stride=1)
        fmap_size = self.features[0].shape[-2]         # Feature map sizes h, w
        self.resize = torch.nn.AdaptiveAvgPool2d(fmap_size)

        resized_maps = [self.resize(self.avg(fmap)) for fmap in self.features]
        patch = torch.cat(resized_maps, 1)            # Merge the resized feature maps
        patch = patch.reshape(patch.shape[1], -1).T   # Craete a column tensor

        return patch
#Model_Download
image = Image.open(r"datasets/metal_nut/test/color/000.png")
image = transform(image).unsqueeze(0).cuda()

backbone = resnet_feature_extractor().cuda()
feature = backbone(image)

# print(backbone.features[0].shape)
# print(backbone.features[1].shape)

print(feature.shape)

# plt.imshow(image[0].permute(1,2,0))

Adding the contents to memory bank 
memory_bank =[]

folder_path = Path(r'datasets/metal_nut/train/good')

for pth in tqdm(folder_path.iterdir(),leave=False):

    with torch.no_grad():
        data = transform(Image.open(pth)).cuda().unsqueeze(0)
        features = backbone(data)
        memory_bank.append(features.cpu().detach())

memory_bank = torch.cat(memory_bank,dim=0).cuda()

selected_indices = np.random.choice(len(memory_bank), size=len(memory_bank)//10, replace=False)
memory_bank = memory_bank[selected_indices]

y_score=[]
folder_path = Path(r'datasets/metal_nut/train/good')

for pth in tqdm(folder_path.iterdir(),leave=False):
    data = transform(Image.open(pth)).cuda().unsqueeze(0)
    with torch.no_grad():
        features = backbone(data)
    distances = torch.cdist(features, memory_bank, p=2.0)
    dist_score, dist_score_idxs = torch.min(distances, dim=1)
    s_star = torch.max(dist_score)
    segm_map = dist_score.view(1, 1, 28, 28)

    y_score.append(s_star.cpu().numpy())

best_threshold = np.mean(y_score) + 2 * np.std(y_score)

plt.hist(y_score,bins=50)
plt.vlines(x=best_threshold,ymin=0,ymax=30,color='r')
plt.show()


y_score = []
y_true=[]

for classes in ['color','good','bent','scratch','flip']:
    folder_path = Path(r'datasets/metal_nut/test/{}'.format(classes))

    for pth in tqdm(folder_path.iterdir(),leave=False):

        class_label = pth.parts[-2]
        with torch.no_grad():
            test_image = transform(Image.open(pth)).cuda().unsqueeze(0)
            features = backbone(test_image)

        distances = torch.cdist(features, memory_bank, p=2.0)
        dist_score, dist_score_idxs = torch.min(distances, dim=1)
        s_star = torch.max(dist_score)
        segm_map = dist_score.view(1, 1, 28, 28)

        y_score.append(s_star.cpu().numpy())
        y_true.append(0 if class_label == 'good' else 1)

# plotting the y_score values which do not belong to 'good' class

y_score_nok = [score  for score,true in zip(y_score,y_true) if true==1]
plt.hist(y_score_nok,bins=50)
plt.vlines(x=best_threshold,ymin=0,ymax=30,color='r')
plt.show()


test_image = transform(Image.open(r'datasets/metal_nut/test/color/000.png')).cuda().unsqueeze(0)
features = backbone(test_image)

distances = torch.cdist(features, memory_bank, p=2.0)
dist_score, dist_score_idxs = torch.min(distances, dim=1)
s_star = torch.max(dist_score)
segm_map = dist_score.view(1, 1, 28, 28)

segm_map = torch.nn.functional.interpolate(     # Upscale by bi-linaer interpolation to match the original input resolution
                segm_map,
                size=(224, 224),
                mode='bilinear'
            )

plt.imshow(segm_map.cpu().squeeze(), cmap='jet')

from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, ConfusionMatrixDisplay, f1_score


# Calculate AUC-ROC score
auc_roc_score = roc_auc_score(y_true, y_score)
print("AUC-ROC Score:", auc_roc_score)

# Plot ROC curve
fpr, tpr, thresholds = roc_curve(y_true, y_score)
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % auc_roc_score)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

f1_scores = [f1_score(y_true, y_score >= threshold) for threshold in thresholds]

# Select the best threshold based on F1 score
best_threshold = thresholds[np.argmax(f1_scores)]

print(f'best_threshold = {best_threshold}')

# Generate confusion matrix
cm = confusion_matrix(y_true, (y_score >= best_threshold).astype(int))
disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=['OK','NOK'])
disp.plot()
plt.show()

auc_roc_score = roc_auc_score(y_true, y_score)
print("AUC-ROC Score:", auc_roc_score)

# Plot ROC curve
fpr, tpr, thresholds = roc_curve(y_true, y_score)
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % auc_roc_score)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

f1_scores = [f1_score(y_true, y_score >= threshold) for threshold in thresholds]

# Select the best threshold based on F1 score
best_threshold = thresholds[np.argmax(f1_scores)]

print(f'best_threshold = {best_threshold}')

# Generate confusion matrix
cm = confusion_matrix(y_true, (y_score >= best_threshold).astype(int))
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['OK', 'NOK'])
disp.plot()
plt.show()


test_path = Path('datasets/metal_nut/test')

for path in test_path.glob('*/*.png'):

    fault_type = path.parts[-2]

    if fault_type in ['bent','flip','scratch']:  # or any other fault types you want to include

        test_image = transform(Image.open(path)).cuda().unsqueeze(0)

        with torch.no_grad():
            features = backbone(test_image)
        # Forward pass
        distances = torch.cdist(features, memory_bank, p=2.0)
        dist_score, dist_score_idxs = torch.min(distances, dim=1)
        s_star = torch.max(dist_score)
        segm_map = dist_score.view(1, 1, 28, 28)
        segm_map = torch.nn.functional.interpolate(     # Upscale by bi-linaer interpolation to match the original input resolution
                    segm_map,
                    size=(224, 224),
                    mode='bilinear'
                )
        # Move segm_map to CPU and squeeze to remove extra dimensions
        segm_map = segm_map.cpu().squeeze().numpy()

        y_score_image = s_star.cpu().numpy()

        y_pred_image = 1*(y_score_image >= best_threshold)
        class_label = ['OK','NOK']


        plt.figure(figsize=(15,5))

        plt.subplot(1,3,1)
        plt.imshow(test_image.squeeze().permute(1,2,0).cpu().numpy())
        plt.title(f'fault type: {fault_type}')

        plt.subplot(1,3,2)
        heat_map = segm_map
        plt.imshow(heat_map, cmap='jet',vmin=best_threshold, vmax=best_threshold*2)
        plt.title(f'Anomaly score: {y_score_image / best_threshold:0.4f} || {class_label[y_pred_image]}')

        plt.subplot(1,3,3)
        plt.imshow((heat_map > best_threshold*1.25).astype(np.uint8), cmap='gray')
        plt.title(f'segmentation map')
        plt.show()
        time.sleep(0.05)

for path in test_path.glob('*/*.png'):

    fault_type = path.parts[-2]

    if fault_type in ['good']:  # or any other fault types you want to include

        test_image = transform(Image.open(path)).cuda().unsqueeze(0)

        with torch.no_grad():
            features = backbone(test_image)
        # Forward pass
        distances = torch.cdist(features, memory_bank, p=2.0)
        dist_score, dist_score_idxs = torch.min(distances, dim=1)
        s_star = torch.max(dist_score)
        segm_map = dist_score.view(1, 1, 28, 28)
        segm_map = torch.nn.functional.interpolate(     # Upscale by bi-linaer interpolation to match the original input resolution
                    segm_map,
                    size=(224, 224),
                    mode='bilinear'
                )
        # Move segm_map to CPU and squeeze to remove extra dimensions
        segm_map = segm_map.cpu().squeeze().numpy()

        y_score_image = s_star.cpu().numpy()

        y_pred_image = 1*(y_score_image >= best_threshold)
        class_label = ['OK','NOK']


        plt.figure(figsize=(15,5))

        plt.subplot(1,3,1)
        plt.imshow(test_image.squeeze().permute(1,2,0).cpu().numpy())
        plt.title(f'fault type: {fault_type}')

        plt.subplot(1,3,2)
        heat_map = segm_map
        plt.imshow(heat_map, cmap='jet',vmin=best_threshold, vmax=best_threshold*2)
        plt.title(f'Anomaly score: {y_score_image / best_threshold:0.4f} || {class_label[y_pred_image]}')

        plt.subplot(1,3,3)
        plt.imshow((heat_map > best_threshold*1.25).astype(np.uint8), cmap='gray')
        plt.title(f'segmentation map')
        plt.show()
        time.sleep(0.05)



from sklearn.metrics import classification_report

y_pred = (y_score >= best_threshold).astype(int)

report = classification_report(y_true, y_pred, target_names=['OK', 'NOK'], digits=4)
print(report)
import pandas as pd
report_dict = classification_report(y_true, y_pred, target_names=['OK', 'NOK'], digits=4, output_dict=True)
df_report = pd.DataFrame(report_dict).transpose()
display(df_report)
